#!/usr/bin/env python3
"""
Real-time Speech-to-Text with mlx-whisper.
Modes:
  --single          Ğ¾Ğ´Ğ½Ğ° Ñ„Ñ€Ğ°Ğ·Ğ° â†’ Ğ² Ğ±ÑƒÑ„ĞµÑ€ â†’ Ğ²Ñ‹Ñ…Ğ¾Ğ´
  --single --lang ru Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ñ€ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº
  (Ğ±ĞµĞ· Ñ„Ğ»Ğ°Ğ³Ğ¾Ğ²)      Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼, ÑÑ‚Ğ¾Ğ¿ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ²Ñƒ "exit" / "Ğ²Ñ‹Ñ…Ğ¾Ğ´"
"""

import argparse
import sys
import os
import tempfile
import wave

import mlx_whisper
import pyaudio
import numpy as np
import pyperclip

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ MacBook Pro M5 / 24 GB RAM:
#
#   "mlx-community/whisper-large-v3-turbo"     â€” 809M, ~1.6 GB, â˜… Ğ›Ğ£Ğ§Ğ¨Ğ˜Ğ™ Ğ’Ğ«Ğ‘ĞĞ  â˜…
#       Ğ­Ñ‚Ğ¾ large-v3 Ñ 4 decoder layers Ğ²Ğ¼ĞµÑÑ‚Ğ¾ 32.
#       ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ = large-v2, ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ = tiny/base.
#       Ğ¢ÑĞ¶Ñ‘Ğ»Ñ‹Ğ¹ encoder Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾ Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑÑ Ğ½Ğ° GPU M5.
#       ĞÑ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ÑƒÑÑĞºĞ¸Ğ¹. Ğ›Ğ°Ñ‚ĞµĞ½Ñ†Ğ¸Ñ ~1-2s Ğ½Ğ° Ñ„Ñ€Ğ°Ğ·Ñƒ.
#
# ĞĞ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ²Ñ‹ (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ¾):
#   "mlx-community/whisper-small"              â€” 244M, ~460 MB, ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ»Ğ°Ñ‚ĞµĞ½Ñ†Ğ¸Ñ (<1s)
#   "mlx-community/whisper-large-v3-mlx"       â€” 1.55B, ~3 GB, Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ½Ğ¾ ~5-8s Ğ½Ğ° Ñ„Ñ€Ğ°Ğ·Ñƒ
#   "mlx-community/distil-whisper-large-v3"    â€” 756M, ~1.5 GB, âš ï¸ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹
#
MODEL_NAME = os.environ.get(
    "WHISPER_MODEL",
    "mlx-community/whisper-large-v3-turbo"
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ PyAudio
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FORMAT = pyaudio.paInt16     # 16-bit
CHANNELS = 1                 # Ğ¼Ğ¾Ğ½Ğ¾
RATE = 16000                 # 16 kHz (Whisper Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ ÑÑ‚Ğ¾)
CHUNK = 1024                 # Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±ÑƒÑ„ĞµÑ€Ğ°

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ñ‚Ğ¸ÑˆĞ¸Ğ½Ñ‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SILENCE_THRESHOLD = int(os.environ.get("SILENCE_THRESHOLD", "500"))
SILENCE_DURATION = float(os.environ.get("SILENCE_DURATION", "1.5"))   # ÑĞµĞºÑƒĞ½Ğ´ Ñ‚Ğ¸ÑˆĞ¸Ğ½Ñ‹ = ĞºĞ¾Ğ½ĞµÑ† Ñ„Ñ€Ğ°Ğ·Ñ‹
SILENCE_CHUNKS = int(SILENCE_DURATION * RATE / CHUNK)

# ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ (ÑĞµĞºÑƒĞ½Ğ´Ñ‹) â€” Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… ÑÑ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğ¹
MIN_AUDIO_SECONDS = 0.5


def record_until_silence():
    """Ğ—Ğ°Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°ĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ Ğ¼Ğ¸ĞºÑ€Ğ¾Ñ„Ğ¾Ğ½Ğ° Ğ´Ğ¾ Ğ¿Ğ°ÑƒĞ·Ñ‹ Ğ² Ñ€ĞµÑ‡Ğ¸. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ np.array float32."""
    audio = pyaudio.PyAudio()
    stream = audio.open(
        format=FORMAT, channels=CHANNELS, rate=RATE,
        input=True, frames_per_buffer=CHUNK
    )

    print("ğŸ™  ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€ĞµÑ‡Ğ¸...", file=sys.stderr)

    frames = []
    silent_chunks = 0
    speech_started = False

    try:
        while True:
            data = stream.read(CHUNK, exception_on_overflow=False)
            audio_data = np.frombuffer(data, dtype=np.int16)
            amplitude = np.max(np.abs(audio_data))

            if amplitude >= SILENCE_THRESHOLD:
                if not speech_started:
                    speech_started = True
                    print("ğŸ”´  Ğ—Ğ°Ğ¿Ğ¸ÑÑŒ...", file=sys.stderr)
                silent_chunks = 0
                frames.append(audio_data.astype(np.float32) / 32768.0)
            else:
                if speech_started:
                    frames.append(audio_data.astype(np.float32) / 32768.0)
                    silent_chunks += 1
                    if silent_chunks >= SILENCE_CHUNKS:
                        break
    finally:
        stream.stop_stream()
        stream.close()
        audio.terminate()

    if not frames:
        return None

    audio_array = np.concatenate(frames)
    duration = len(audio_array) / RATE

    if duration < MIN_AUDIO_SECONDS:
        print(f"âš ï¸  Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ°Ñ Ğ·Ğ°Ğ¿Ğ¸ÑÑŒ ({duration:.1f}s), Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞº.", file=sys.stderr)
        return None

    print(f"â¹  Ğ—Ğ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¾ {duration:.1f}s Ğ°ÑƒĞ´Ğ¸Ğ¾.", file=sys.stderr)
    return audio_array


def save_wav(audio_array, path):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ float32 Ğ¼Ğ°ÑÑĞ¸Ğ² Ğ² WAV (Ğ½ÑƒĞ¶ĞµĞ½ Ğ´Ğ»Ñ mlx_whisper.transcribe)."""
    int_data = (audio_array * 32767).astype(np.int16)
    with wave.open(path, 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(2)
        wf.setframerate(RATE)
        wf.writeframes(int_data.tobytes())


def transcribe(audio_array, language=None):
    """Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ñ‘Ñ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ‡ĞµÑ€ĞµĞ· mlx-whisper. Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‚ĞµĞºÑÑ‚."""
    # mlx_whisper.transcribe Ğ¿Ñ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ¿ÑƒÑ‚ÑŒ Ğº Ñ„Ğ°Ğ¹Ğ»Ñƒ Ğ¸Ğ»Ğ¸ numpy array
    # Ğ”Ğ»Ñ Ğ½Ğ°Ğ´Ñ‘Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ WAV
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
        tmp_path = tmp.name
        save_wav(audio_array, tmp_path)

    try:
        kwargs = {"path_or_hf_repo": MODEL_NAME}
        if language:
            kwargs["language"] = language

        result = mlx_whisper.transcribe(tmp_path, **kwargs)
        text = result.get("text", "").strip()
        detected_lang = result.get("language", "?")
        return text, detected_lang
    finally:
        os.unlink(tmp_path)


def main():
    parser = argparse.ArgumentParser(
        description="Real-time STT Ñ‡ĞµÑ€ĞµĞ· mlx-whisper Ğ½Ğ° Apple Silicon"
    )
    parser.add_argument(
        "--single", action="store_true",
        help="ĞĞ´Ğ½Ğ° Ñ„Ñ€Ğ°Ğ·Ğ° â†’ Ğ² Ğ±ÑƒÑ„ĞµÑ€ â†’ Ğ²Ñ‹Ñ…Ğ¾Ğ´"
    )
    parser.add_argument(
        "--lang", type=str, default=None,
        help="ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ·Ñ‹Ğº (ru, en, ka, ...); Ğ±ĞµĞ· Ñ„Ğ»Ğ°Ğ³Ğ° â€” Ğ°Ğ²Ñ‚Ğ¾Ğ´ĞµÑ‚ĞµĞºÑ‚"
    )
    parser.add_argument(
        "--output-file", type=str, default=None,
        help="Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ² Ñ„Ğ°Ğ¹Ğ» (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ --single)"
    )
    parser.add_argument(
        "--no-clipboard", action="store_true",
        help="ĞĞµ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ² Ğ±ÑƒÑ„ĞµÑ€ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ°"
    )
    args = parser.parse_args()

    print(f"ğŸ“¦  ĞœĞ¾Ğ´ĞµĞ»ÑŒ: {MODEL_NAME}", file=sys.stderr)
    print(f"ğŸ”‡  ĞŸĞ¾Ñ€Ğ¾Ğ³ Ñ‚Ğ¸ÑˆĞ¸Ğ½Ñ‹: {SILENCE_THRESHOLD}, Ğ¿Ğ°ÑƒĞ·Ğ°: {SILENCE_DURATION}s", file=sys.stderr)
    if args.lang:
        print(f"ğŸŒ  Ğ¯Ğ·Ñ‹Ğº: {args.lang}", file=sys.stderr)
    else:
        print(f"ğŸŒ  Ğ¯Ğ·Ñ‹Ğº: Ğ°Ğ²Ñ‚Ğ¾Ğ´ĞµÑ‚ĞµĞºÑ‚", file=sys.stderr)
    print("â”€" * 40, file=sys.stderr)

    if args.single:
        # â”€â”€ Ğ ĞµĞ¶Ğ¸Ğ¼ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ„Ñ€Ğ°Ğ·Ñ‹ â”€â”€
        audio = record_until_silence()
        if audio is None:
            print("ĞĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾.", file=sys.stderr)
            sys.exit(1)

        text, lang = transcribe(audio, language=args.lang)
        if not text:
            print("ĞŸÑƒÑÑ‚Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ.", file=sys.stderr)
            sys.exit(1)

        # Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ² stdout (Ğ´Ğ»Ñ Ğ¿Ğ°Ğ¹Ğ¿Ğ¾Ğ²)
        print(text)

        # Ğ’ Ğ±ÑƒÑ„ĞµÑ€ Ğ¾Ğ±Ğ¼ĞµĞ½Ğ°
        if not args.no_clipboard:
            pyperclip.copy(text)
            print(f"ğŸ“‹  Ğ¡ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ² Ğ±ÑƒÑ„ĞµÑ€ (ÑĞ·Ñ‹Ğº: {lang})", file=sys.stderr)

        # Ğ’ Ñ„Ğ°Ğ¹Ğ»
        if args.output_file:
            with open(args.output_file, 'w', encoding='utf-8') as f:
                f.write(text + "\n")
            print(f"ğŸ’¾  Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ Ğ² {args.output_file}", file=sys.stderr)

    else:
        # â”€â”€ ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ â”€â”€
        print("â™¾ï¸  ĞĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼. Ğ¡ĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ 'exit' Ğ¸Ğ»Ğ¸ 'Ğ²Ñ‹Ñ…Ğ¾Ğ´' Ğ´Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸.", file=sys.stderr)
        while True:
            audio = record_until_silence()
            if audio is None:
                continue

            text, lang = transcribe(audio, language=args.lang)
            if not text:
                continue

            print(text)

            if not args.no_clipboard:
                pyperclip.copy(text)
                print(f"ğŸ“‹  [{lang}] â†’ Ğ±ÑƒÑ„ĞµÑ€", file=sys.stderr)

            # Ğ¡Ñ‚Ğ¾Ğ¿-ÑĞ»Ğ¾Ğ²Ğ°
            lower = text.lower().strip().rstrip(".")
            if lower in ("exit", "Ğ²Ñ‹Ñ…Ğ¾Ğ´", "ÑÑ‚Ğ¾Ğ¿", "stop"):
                print("ğŸ‘‹  Ğ—Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğµ.", file=sys.stderr)
                break

            print("â”€" * 40, file=sys.stderr)


if __name__ == "__main__":
    main()
